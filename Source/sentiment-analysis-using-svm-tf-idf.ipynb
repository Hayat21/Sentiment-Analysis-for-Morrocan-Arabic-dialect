{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install demoji","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-21T20:44:43.154696Z","iopub.execute_input":"2022-01-21T20:44:43.155035Z","iopub.status.idle":"2022-01-21T20:44:51.589206Z","shell.execute_reply.started":"2022-01-21T20:44:43.154932Z","shell.execute_reply":"2022-01-21T20:44:51.588402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport os\nimport demoji\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.isri import ISRIStemmer","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:45:59.727858Z","iopub.execute_input":"2022-01-21T20:45:59.728174Z","iopub.status.idle":"2022-01-21T20:45:59.739431Z","shell.execute_reply.started":"2022-01-21T20:45:59.728134Z","shell.execute_reply":"2022-01-21T20:45:59.735829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"train.csv\")\ndf_test=pd.read_csv(\"test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:46:01.324978Z","iopub.execute_input":"2022-01-21T20:46:01.32554Z","iopub.status.idle":"2022-01-21T20:46:01.38402Z","shell.execute_reply.started":"2022-01-21T20:46:01.325497Z","shell.execute_reply":"2022-01-21T20:46:01.383307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('punkt')\nstop_words = stopwords.words('arabic')\ntranslator = str.maketrans('', '', string.punctuation)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:46:05.647974Z","iopub.execute_input":"2022-01-21T20:46:05.648799Z","iopub.status.idle":"2022-01-21T20:46:05.797394Z","shell.execute_reply.started":"2022-01-21T20:46:05.64874Z","shell.execute_reply":"2022-01-21T20:46:05.796624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeStopWords(text):\n    word_tokens = word_tokenize(text) \n    filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n    text = ' '.join([i for i in filtered_sentence])\n    return text\ndef NormalizeArabic(text):\n    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    return text\ndef arabic_diacritics(text):\n    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(arabic_diacritics, '', text)\n    return text\n\ndef removeNumbers(text):\n    \"\"\" Removes integers \"\"\"\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\ndef stemming(text):\n    st = ISRIStemmer()\n    stemmed_words = []\n    word_tokens = word_tokenize(text) \n    for w in word_tokens:\n        stemmed_words.append(st.stem(w))\n    stemmed_words = \" \".join(stemmed_words)\n    return stemmed_words\n\ndef remove_english_characters(text):\n        return re.sub(r'[a-zA-Z]+', '', text)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:46:09.616112Z","iopub.execute_input":"2022-01-21T20:46:09.616648Z","iopub.status.idle":"2022-01-21T20:46:09.62858Z","shell.execute_reply.started":"2022-01-21T20:46:09.61661Z","shell.execute_reply":"2022-01-21T20:46:09.627805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in df_train.iterrows():\n    row['comment'] = removeStopWords(row['comment'])\n    row['comment'] = NormalizeArabic(row['comment'])\n    row['comment'] = arabic_diacritics(row['comment'])\n    row['comment'] = removeNumbers(row['comment'])\n    row['comment'] = row['comment'].translate(translator)\n    row['comment'] = stemming(row['comment'])\n    new_df = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n    df_train.update(new_df)\n#remove emoji     \ndf_train['comment']=df_train['comment'].apply(lambda x: demoji.replace(x,\"\"))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:46:11.934332Z","iopub.execute_input":"2022-01-21T20:46:11.935105Z","iopub.status.idle":"2022-01-21T20:46:19.563848Z","shell.execute_reply.started":"2022-01-21T20:46:11.935058Z","shell.execute_reply":"2022-01-21T20:46:19.563096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split,GridSearchCV\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df_train['comment'], df_train['label'], df_train.index, test_size=0.20, random_state=20)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:46:22.646728Z","iopub.execute_input":"2022-01-21T20:46:22.646979Z","iopub.status.idle":"2022-01-21T20:46:22.655725Z","shell.execute_reply.started":"2022-01-21T20:46:22.64695Z","shell.execute_reply":"2022-01-21T20:46:22.655056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, encoding='latin-1',norm='l2', ngram_range=(1,2))\nfeatures = tfidf.fit_transform(df_train['comment']).toarray()\nlabels = df_train['label']","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:46:24.375054Z","iopub.execute_input":"2022-01-21T20:46:24.375335Z","iopub.status.idle":"2022-01-21T20:46:24.801088Z","shell.execute_reply.started":"2022-01-21T20:46:24.375305Z","shell.execute_reply":"2022-01-21T20:46:24.800337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC  \nmodel=SVC(C= 10, gamma=1, kernel='sigmoid')\nclf=model.fit(features,labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T20:56:30.250818Z","iopub.execute_input":"2022-01-21T20:56:30.251272Z","iopub.status.idle":"2022-01-21T21:00:14.12042Z","shell.execute_reply.started":"2022-01-21T20:56:30.251233Z","shell.execute_reply":"2022-01-21T21:00:14.11965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_test['comment']\ndf_unseen = pd.DataFrame(data=data)\ndf_unseen_1 = pd.DataFrame(data=data)\nfor index, row in df_unseen.iterrows():\n    row['comment'] = removeStopWords(row['comment'])\n    row['comment'] = NormalizeArabic(row['comment'])\n    row['comment'] = arabic_diacritics(row['comment'])\n    row['comment'] = removeNumbers(row['comment'])\n    row['comment'] = row['comment'].translate(translator)\n    row['comment'] = stemming(row['comment'])\n    new_df = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n    df_unseen.update(new_df)\ntxt= tfidf.transform(df_unseen['comment']).toarray()\n   ","metadata":{"execution":{"iopub.status.busy":"2022-01-21T21:01:32.310325Z","iopub.execute_input":"2022-01-21T21:01:32.311077Z","iopub.status.idle":"2022-01-21T21:01:32.935133Z","shell.execute_reply.started":"2022-01-21T21:01:32.311029Z","shell.execute_reply":"2022-01-21T21:01:32.9343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_columns = ['id','label']\ncsv_file = \"Sample_Submission.csv\"\ny_pred = clf.predict(txt)\nids = [i for i in range(1,241)]\nids = pd.DataFrame(data=ids)\npred = pd.DataFrame(data=y_pred)\ndata=pd.concat([ids,pred],axis=1)\nprint(data)\ndata.to_csv(csv_file,sep=',',header=['id','label'],index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T21:02:44.998863Z","iopub.execute_input":"2022-01-21T21:02:44.999443Z","iopub.status.idle":"2022-01-21T21:03:12.026547Z","shell.execute_reply.started":"2022-01-21T21:02:44.999403Z","shell.execute_reply":"2022-01-21T21:03:12.02579Z"},"trusted":true},"execution_count":null,"outputs":[]}]}